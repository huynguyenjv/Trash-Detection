#!/usr/bin/env python3
"""
Quick Start Script cho Multi-Dataset Processing
Thiáº¿t láº­p nhanh mÃ´i trÆ°á»ng vÃ  cháº¡y gá»™p dataset

Author: Huy Nguyen  
Date: August 2025
"""

import os
import sys
import json
from pathlib import Path
import subprocess

def check_requirements():
    """Kiá»ƒm tra cÃ¡c requirements cáº§n thiáº¿t"""
    print("ğŸ” Kiá»ƒm tra requirements...")
    
    # Kiá»ƒm tra cÃ¡c package cÆ¡ báº£n trÆ°á»›c
    basic_packages = ['yaml', 'tqdm']
    missing_packages = []
    
    for package in basic_packages:
        try:
            __import__(package)
            print(f"âœ… {package}")
        except ImportError:
            missing_packages.append(package)
            print(f"âŒ {package}")
    
    # Kiá»ƒm tra kaggle riÃªng biá»‡t (khÃ´ng authenticate)
    try:
        import importlib.util
        spec = importlib.util.find_spec("kaggle")
        if spec is not None:
            print("âœ… kaggle (package)")
        else:
            missing_packages.append('kaggle')
            print("âŒ kaggle")
    except Exception:
        missing_packages.append('kaggle')
        print("âŒ kaggle")
    
    if missing_packages:
        print(f"\nâš ï¸  Thiáº¿u cÃ¡c package: {', '.join(missing_packages)}")
        print("CÃ i Ä‘áº·t báº±ng: pip install " + " ".join(missing_packages))
        return False
    
    return True


def setup_kaggle_api():
    """Thiáº¿t láº­p Kaggle API"""
    print("\nğŸ”‘ Thiáº¿t láº­p Kaggle API...")
    
    # Windows: thá»­ cáº£ hai vá»‹ trÃ­
    kaggle_locations = [
        Path.home() / ".kaggle" / "kaggle.json",    # Linux/Mac style
        Path.home() / ".config" / "kaggle" / "kaggle.json"  # Standard location
    ]
    
    kaggle_file = None
    for location in kaggle_locations:
        if location.exists():
            kaggle_file = location
            break
    
    if kaggle_file:
        print(f"âœ… Kaggle API key Ä‘Ã£ tá»“n táº¡i táº¡i: {kaggle_file}")
        return True
    
    print("âŒ ChÆ°a cÃ³ Kaggle API key")
    print("\nHÆ°á»›ng dáº«n thiáº¿t láº­p:")
    print("1. ÄÄƒng nháº­p vÃ o https://kaggle.com")
    print("2. VÃ o Account â†’ API â†’ Create New API Token") 
    print("3. Download file kaggle.json")
    print("4. Äáº·t file vÃ o má»™t trong cÃ¡c vá»‹ trÃ­:")
    for location in kaggle_locations:
        print(f"   - {location}")
    
    setup_now = input("\nBáº¡n muá»‘n thiáº¿t láº­p ngay khÃ´ng? (y/n): ").lower()
    
    if setup_now == 'y':
        # Chá»n vá»‹ trÃ­ Ä‘áº·t file (dÃ¹ng vá»‹ trÃ­ Ä‘áº§u tiÃªn)
        kaggle_dir = kaggle_locations[0].parent
        kaggle_file = kaggle_locations[0]
        
        # Táº¡o thÆ° má»¥c
        kaggle_dir.mkdir(parents=True, exist_ok=True)
        
        # Nháº­p thÃ´ng tin
        username = input("Kaggle username: ").strip()
        api_key = input("Kaggle API key: ").strip()
        
        if username and api_key:
            # Táº¡o file
            kaggle_config = {"username": username, "key": api_key}
            with open(kaggle_file, 'w') as f:
                json.dump(kaggle_config, f)
            
            # Set permissions (Unix only)
            if os.name != 'nt':  # KhÃ´ng pháº£i Windows
                os.chmod(kaggle_file, 0o600)
            
            print(f"âœ… ÄÃ£ thiáº¿t láº­p Kaggle API táº¡i: {kaggle_file}")
            return True
    
    return False


def create_directory_structure():
    """Táº¡o cáº¥u trÃºc thÆ° má»¥c cáº§n thiáº¿t"""
    print("\nğŸ“ Táº¡o cáº¥u trÃºc thÆ° má»¥c...")
    
    directories = [
        "source_datasets",
        "merged_dataset", 
        "logs"
    ]
    
    for dir_name in directories:
        Path(dir_name).mkdir(exist_ok=True)
        print(f"âœ… {dir_name}/")
    
    return True


def show_dataset_info():
    """Hiá»ƒn thá»‹ thÃ´ng tin vá» datasets sáº½ download"""
    print("\nğŸ“Š Datasets sáº½ Ä‘Æ°á»£c download:")
    
    datasets = [
        ("arkadiyhacks/drinking-waste-classification", "~50MB"),
        ("youssefelebiary/household-trash-recycling-dataset", "~200MB"),
        ("vencerlanz09/taco-dataset-yolo-format", "~500MB"), 
        ("spellsharp/garbage-data", "~100MB")
    ]
    
    total_size = "~850MB"
    
    for i, (dataset, size) in enumerate(datasets, 1):
        print(f"  {i}. {dataset} ({size})")
    
    print(f"\nTá»•ng dung lÆ°á»£ng Æ°á»›c tÃ­nh: {total_size}")
    print("Thá»i gian download: 5-15 phÃºt tÃ¹y vÃ o tá»‘c Ä‘á»™ máº¡ng")


def run_preprocessing():
    """Cháº¡y quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½"""
    print("\nğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh gá»™p dataset...")
    
    try:
        # Import vÃ  cháº¡y
        import sys
        from pathlib import Path
        sys.path.append(str(Path(__file__).parent / "src"))
        
        from data_preprocessing import MultiDatasetConfig, MultiDatasetProcessor
        
        config = MultiDatasetConfig()
        processor = MultiDatasetProcessor(config)
        
        # Há»i cÃ³ download khÃ´ng
        download = input("Download datasets tá»« Kaggle? (y/n): ").lower()
        if download == 'y':
            try:
                print("â¬‡ï¸  Äang download datasets...")
                processor.download_datasets()
            except Exception as e:
                print(f"âŒ Lá»—i download: {e}")
                print("ğŸ’¡ Gá»£i Ã½: Báº¡n cÃ³ thá»ƒ bá» qua download vÃ  dÃ¹ng datasets cÃ³ sáºµn")
                continue_choice = input("Tiáº¿p tá»¥c mÃ  khÃ´ng download? (y/n): ").lower()
                if continue_choice != 'y':
                    return False
        
        # Gá»™p datasets
        print("ğŸ”§ Äang gá»™p datasets...")
        processor.process_all_datasets()
        
        print("\nâœ… HOÃ€N THÃ€NH!")
        print(f"ğŸ“ Dataset Ä‘Ã£ gá»™p táº¡i: {config.output_dataset_path}")
        print(f"ğŸ“‹ BÃ¡o cÃ¡o táº¡i: {config.output_dataset_path}/dataset_summary.json")
        
        return True
        
    except ImportError:
        print("âŒ KhÃ´ng thá»ƒ import data_preprocessing module")
        print("HÃ£y Ä‘áº£m báº£o file data_preprocessing.py á»Ÿ trong thÆ° má»¥c src/")
        return False
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        return False


def show_next_steps():
    """Hiá»ƒn thá»‹ cÃ¡c bÆ°á»›c tiáº¿p theo"""
    print("\nğŸ¯ BÆ°á»›c tiáº¿p theo:")
    print("1. Kiá»ƒm tra dataset Ä‘Ã£ gá»™p trong thÆ° má»¥c merged_dataset/")
    print("2. Xem bÃ¡o cÃ¡o trong dataset_summary.json")
    print("3. Sá»­ dá»¥ng data.yaml Ä‘á»ƒ train YOLOv8:")
    print("   ```python")
    print("   from ultralytics import YOLO")
    print("   model = YOLO('yolov8n.pt')")
    print("   model.train(data='merged_dataset/data.yaml', epochs=100)")
    print("   ```")


def main():
    """HÃ m main"""
    print("ğŸ—‚ï¸  QUICK START - MULTI-DATASET PROCESSING")
    print("=" * 50)
    
    # Kiá»ƒm tra requirements
    if not check_requirements():
        print("\nâŒ Vui lÃ²ng cÃ i Ä‘áº·t missing packages trÆ°á»›c")
        return
    
    # Thiáº¿t láº­p Kaggle API
    if not setup_kaggle_api():
        print("\nâš ï¸  CÃ³ thá»ƒ bá» qua náº¿u datasets Ä‘Ã£ cÃ³ sáºµn")
    
    # Táº¡o thÆ° má»¥c
    create_directory_structure()
    
    # Hiá»ƒn thá»‹ thÃ´ng tin datasets
    show_dataset_info()
    
    # Cháº¡y preprocessing
    proceed = input("\nğŸš€ Báº¡n cÃ³ muá»‘n tiáº¿p tá»¥c? (y/n): ").lower()
    if proceed == 'y':
        if run_preprocessing():
            show_next_steps()
        else:
            print("\nâŒ QuÃ¡ trÃ¬nh tháº¥t báº¡i. Vui lÃ²ng kiá»ƒm tra logs.")
    else:
        print("\nâœ‹ Dá»«ng láº¡i. Báº¡n cÃ³ thá»ƒ cháº¡y láº¡i script nÃ y báº¥t cá»© lÃºc nÃ o.")
    
    print("\nğŸ“š Xem thÃªm hÆ°á»›ng dáº«n chi tiáº¿t trong DATASET_MERGING_GUIDE.md")


if __name__ == "__main__":
    main()
