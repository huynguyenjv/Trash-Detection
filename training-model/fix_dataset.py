#!/usr/bin/env python3
"""
Script s·ª≠a ch·ªØa dataset detection
"""
import os
import yaml
import shutil
from pathlib import Path
from collections import defaultdict
import random

def fix_dataset():
    """S·ª≠a ch·ªØa dataset detection"""
    
    dataset_path = Path("data/processed/detection")
    
    print("üîß FIXING DATASET ISSUES")
    print("="*50)
    
    # 1. Fix mismatch gi·ªØa images v√† labels
    splits = ['train', 'val', 'test']
    for split in splits:
        img_dir = dataset_path / f"images/{split}"
        label_dir = dataset_path / f"labels/{split}"
        
        if not (img_dir.exists() and label_dir.exists()):
            continue
            
        # L·∫•y list images v√† labels
        img_files = set()
        for ext in ['*.jpg', '*.jpeg', '*.png']:
            img_files.update([f.stem for f in img_dir.glob(ext)])
        
        label_files = set([f.stem for f in label_dir.glob('*.txt')])
        
        print(f"üìÇ {split.upper()}:")
        print(f"   Images: {len(img_files)}")
        print(f"   Labels: {len(label_files)}")
        
        # Remove orphan labels
        orphan_labels = label_files - img_files
        if orphan_labels:
            print(f"   üóëÔ∏è  Removing {len(orphan_labels)} orphan labels")
            for label in orphan_labels:
                (label_dir / f"{label}.txt").unlink()
        
        # Remove images without labels
        missing_labels = img_files - label_files
        if missing_labels:
            print(f"   üóëÔ∏è  Removing {len(missing_labels)} images without labels")
            for img in missing_labels:
                for ext in ['jpg', 'jpeg', 'png']:
                    img_file = img_dir / f"{img}.{ext}"
                    if img_file.exists():
                        img_file.unlink()
                        break
    
    # 2. Ph√¢n t√≠ch l·∫°i sau khi fix
    print("\nüìä DATASET AFTER FIXING:")
    print("="*30)
    
    config_file = dataset_path / "dataset.yaml"
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
    
    for split in splits:
        img_dir = dataset_path / f"images/{split}"
        label_dir = dataset_path / f"labels/{split}"
        
        if img_dir.exists() and label_dir.exists():
            img_count = len(list(img_dir.glob('*.jpg'))) + len(list(img_dir.glob('*.png')))
            label_count = len(list(label_dir.glob('*.txt')))
            
            print(f"{split}: {img_count} images, {label_count} labels")

def create_balanced_config():
    """T·∫°o config training ƒë·ªÉ handle class imbalance"""
    
    # Class weights ƒë·ªÉ balance classes
    # Class "other" c√≥ weight th·∫•p, c√°c class kh√°c c√≥ weight cao
    class_weights = {
        0: 5.0,   # cardboard (2%)
        1: 8.0,   # glass (1%) 
        2: 3.0,   # metal (4%)
        3: 10.0,  # organic (1%)
        4: 0.5,   # other (60%) - weight th·∫•p
        5: 4.0,   # paper (3%)
        6: 1.5    # plastic (27%)
    }
    
    # T·∫°o config m·ªõi v·ªõi focus loss v√† class weights
    balanced_config = {
        # Model config
        'model': 'yolov8n.pt',
        'epochs': 300,
        'batch': 8,
        'imgsz': 640,
        'device': 0,
        
        # Learning
        'optimizer': 'SGD',
        'lr0': 0.01,
        'lrf': 0.1,
        'momentum': 0.937,
        'weight_decay': 0.0005,
        'warmup_epochs': 3.0,
        'warmup_momentum': 0.8,
        'warmup_bias_lr': 0.1,
        
        # Loss weights ƒë·ªÉ handle imbalance
        'box': 7.5,
        'cls': 1.0,
        'dfl': 1.5,
        
        # Detection thresholds
        'conf': 0.001,
        'iou': 0.7,
        
        # Augmentation - reduced ƒë·ªÉ tr√°nh overfitting tr√™n class thi·ªÉu s·ªë
        'hsv_h': 0.005,
        'hsv_s': 0.3,
        'hsv_v': 0.2,
        'degrees': 3.0,
        'translate': 0.05,
        'scale': 0.25,
        'shear': 1.0,
        'perspective': 0.0001,
        'flipud': 0.0,
        'fliplr': 0.5,
        'mosaic': 0.8,  # Gi·ªØ mosaic ƒë·ªÉ tƒÉng diversity
        'mixup': 0.2,   # Th√™m mixup
        'copy_paste': 0.1,
        
        # Early stopping
        'patience': 50,
        'save_period': 10,
        
        # Validation
        'val': True,
        'split': 'val',
        'save_json': True,
        'save_hybrid': False,
        'half': False,
        'dnn': False,
        
        # Class weights
        'cls_weights': class_weights
    }
    
    # L∆∞u config m·ªõi
    with open('training_config_balanced.yaml', 'w') as f:
        yaml.dump(balanced_config, f, default_flow_style=False, indent=2)
    
    print("üíæ ƒê√£ t·∫°o training_config_balanced.yaml v·ªõi class weights")
    
    return balanced_config

def suggest_improvements():
    """ƒê∆∞a ra c√°c khuy·∫øn ngh·ªã c·∫£i thi·ªán"""
    
    print("\nüí° KHUY·∫æN NGH·ªä C·∫¢I THI·ªÜN DATASET:")
    print("="*40)
    
    print("1. üéØ CLASS IMBALANCE:")
    print("   - S·ª≠ d·ª•ng config balanced v·ªõi class weights")
    print("   - TƒÉng augmentation cho minority classes")
    print("   - S·ª≠ d·ª•ng focal loss thay cross-entropy")
    
    print("\n2. üì∏ DATA COLLECTION:")
    print("   - Thu th·∫≠p th√™m data cho: glass, organic, cardboard")
    print("   - Gi·∫£m s·ªë l∆∞·ª£ng class 'other' ho·∫∑c chia nh·ªè ra")
    print("   - ƒê·∫£m b·∫£o quality annotations")
    
    print("\n3. üîß TRAINING STRATEGY:")
    print("   - Train v·ªõi learning rate th·∫•p (0.001-0.005)")
    print("   - S·ª≠ d·ª•ng transfer learning t·ª´ pretrained model")
    print("   - √Åp d·ª•ng progressive resizing")
    print("   - Monitor validation loss carefully")
    
    print("\n4. üìä EVALUATION:")
    print("   - S·ª≠ d·ª•ng per-class metrics")
    print("   - Focus v√†o recall cho minority classes")
    print("   - Confusion matrix analysis")

if __name__ == "__main__":
    fix_dataset()
    create_balanced_config()
    suggest_improvements()